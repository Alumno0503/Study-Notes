{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#  HKU CIVL7024  Assignment 2\n",
    "\n",
    "## Name:YAN,Yiting Uni. No.:3036494960\n",
    "\n",
    "## Part I Image Processing\n",
    "The goal of this part of the assignment is to stitch the two images together.(This notebook (Part I) requires the `tung_ping_chau_view1.jpeg` and `tung_ping_chau_view2.jpeg` files to be placed in the same directory level.)\n",
    "\n",
    "1.Load the images and convert to grayscale. Explain, in the notebook, why do we want to do so."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install opencv-python opencv-contrib-python matplotlib numpy\n",
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load the images\n",
    "img1 = cv2.imread('tung_ping_chau_view1.jpeg')\n",
    "img2 = cv2.imread('tung_ping_chau_view2.jpeg')\n",
    "\n",
    "# Check if images are loaded correctly\n",
    "if img1 is None:\n",
    "    print(\"Error: Could not load 'tung_ping_chau_view1.jpeg'\")\n",
    "    # Try alternative filename\n",
    "    img1 = cv2.imread('tung_ping_Chau_view1.jpg')\n",
    "\n",
    "if img2 is None:\n",
    "    print(\"Error: Could not load 'tung_ping_chau_view2.jpeg'\")\n",
    "    # Try alternative filename\n",
    "    img2 = cv2.imread('tung_ping_Chau_view2.jpg')\n",
    "\n",
    "# Convert to grayscale\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Display both original and grayscale images\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Display original images\n",
    "axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Image 1 (Original)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title('Image 2 (Original)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Display grayscale images\n",
    "axes[1, 0].imshow(gray1, cmap='gray')\n",
    "axes[1, 0].set_title('Image 1 (Grayscale)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "axes[1, 1].imshow(gray2, cmap='gray')\n",
    "axes[1, 1].set_title('Image 2 (Grayscale)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"Original Image 1 shape: {img1.shape}\")\n",
    "print(f\"Grayscale Image 1 shape: {gray1.shape}\")\n",
    "print(f\"Original Image 2 shape: {img2.shape}\")\n",
    "print(f\"Grayscale Image 2 shape: {gray2.shape}\")\n"
   ],
   "id": "fb6430754505c0ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Why Convert to Grayscale?\n",
    "\n",
    "1. **Computational Efficiency**：Grayscale images have only one channel, while color images have three channels (RGB). Processing grayscale images requires only one-third of the computational load compared to color images.\n",
    "\n",
    "2. **Feature Detection Effectiveness**：Most feature detection algorithms (such as Harris, SIFT, and SURF) are designed for single-channel images. Grayscale images contain brightness information, which is crucial for edge and corner detection.\n",
    "\n",
    "3. **Reduced Noise Impact**：Color information may introduce additional noise. Converting to grayscale simplifies the image, making features more stable.\n",
    "\n",
    "4. **Algorithm Compatibility**：Many feature detection functions in OpenCV require input in grayscale format."
   ],
   "id": "aaa931100d429de0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2.Use the Harris detector to detect the corner points. Writing your own detector is recommended as good practice, but using the OpenCV library is also fine.",
   "id": "d299e48e8fc64360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Harris Corner Detection (Custom Implementation)\n",
    "def harris_corner_detector(image, k=0.04, threshold_ratio=0.01):\n",
    "    \"\"\"\n",
    "    Custom Harris Corner Detector\n",
    "\n",
    "    Parameters:\n",
    "    image: Input image\n",
    "    k: Harris detector free parameter (typically 0.04-0.06)\n",
    "    threshold_ratio: Corner response threshold ratio (0.01 = 1% of max)\n",
    "\n",
    "    Returns:\n",
    "    corners: Binary image with corners marked\n",
    "    corner_coords: Coordinates of detected corners (limited to 500)\n",
    "    \"\"\"\n",
    "    # 1. Calculate gradients in x and y directions\n",
    "    Ix = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    Iy = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)\n",
    "\n",
    "    # 2. Calculate gradient products\n",
    "    Ixx = Ix * Ix\n",
    "    Iyy = Iy * Iy\n",
    "    Ixy = Ix * Iy\n",
    "\n",
    "    # 3. Apply Gaussian filtering to weight gradient products\n",
    "    kernel_size = 3\n",
    "    sigma = 1.5\n",
    "    Ixx = cv2.GaussianBlur(Ixx, (kernel_size, kernel_size), sigma)\n",
    "    Iyy = cv2.GaussianBlur(Iyy, (kernel_size, kernel_size), sigma)\n",
    "    Ixy = cv2.GaussianBlur(Ixy, (kernel_size, kernel_size), sigma)\n",
    "\n",
    "    # 4. Calculate Harris response function\n",
    "    det = Ixx * Iyy - Ixy * Ixy\n",
    "    trace = Ixx + Iyy\n",
    "    R = det - k * (trace ** 2)\n",
    "\n",
    "    # 5. Normalize response to 0-255\n",
    "    R_norm = cv2.normalize(R, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
    "\n",
    "    # 6. Thresholding\n",
    "    threshold_value = threshold_ratio * 255\n",
    "    corners_binary = np.zeros_like(image, dtype=np.uint8)\n",
    "    corners_binary[R_norm > threshold_value] = 255\n",
    "\n",
    "    # 7. Non-maximum suppression (using dilation to find local maxima)\n",
    "    # Create a kernel for dilation\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    dilated = cv2.dilate(R_norm, kernel)\n",
    "\n",
    "    # Local maxima are points where the original value equals the dilated value\n",
    "    local_maxima = (R_norm == dilated)\n",
    "\n",
    "    # Combine threshold and local maxima\n",
    "    corners_final = np.zeros_like(image, dtype=np.uint8)\n",
    "    corners_final[(R_norm > threshold_value) & local_maxima] = 255\n",
    "\n",
    "    # 8. Extract corner coordinates\n",
    "    corner_coords = np.column_stack(np.where(corners_final > 0))\n",
    "\n",
    "    # 9. Sort corners by response strength (descending)\n",
    "    if len(corner_coords) > 0:\n",
    "        # Get response values for each corner\n",
    "        responses = R_norm[corner_coords[:, 0], corner_coords[:, 1]]\n",
    "        # Sort by response (descending)\n",
    "        sorted_indices = np.argsort(responses)[::-1]\n",
    "        corner_coords = corner_coords[sorted_indices]\n",
    "\n",
    "    # Return only first 500 corners (or all if less than 500)\n",
    "    max_corners = min(500, len(corner_coords))\n",
    "\n",
    "    return corners_final, corner_coords[:max_corners]\n",
    "\n",
    "# Apply Harris detector with adjusted parameters\n",
    "harris1, corners1 = harris_corner_detector(gray1, k=0.05, threshold_ratio=0.02)\n",
    "harris2, corners2 = harris_corner_detector(gray2, k=0.05, threshold_ratio=0.02)\n",
    "\n",
    "print(f\"Number of Harris corners detected in image 1: {len(corners1)}\")\n",
    "print(f\"Number of Harris corners detected in image 2: {len(corners2)}\")\n",
    "\n",
    "# Display Harris detection results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Image 1 with corners\n",
    "axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "if len(corners1) > 0:\n",
    "    axes[0, 0].scatter(corners1[:, 1], corners1[:, 0], s=10, c='r', marker='o', alpha=0.6)\n",
    "axes[0, 0].set_title(f'Image 1 with Harris Corners ({len(corners1)} corners)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "# Harris response 1\n",
    "axes[0, 1].imshow(harris1, cmap='hot')\n",
    "axes[0, 1].set_title('Harris Response (Image 1)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Image 2 with corners\n",
    "axes[1, 0].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "if len(corners2) > 0:\n",
    "    axes[1, 0].scatter(corners2[:, 1], corners2[:, 0], s=10, c='r', marker='o', alpha=0.6)\n",
    "axes[1, 0].set_title(f'Image 2 with Harris Corners ({len(corners2)} corners)')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Harris response 2\n",
    "axes[1, 1].imshow(harris2, cmap='hot')\n",
    "axes[1, 1].set_title('Harris Response (Image 2)')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Show corner statistics\n",
    "if len(corners1) > 0:\n",
    "    print(f\"\\nImage 1 corner statistics:\")\n",
    "    print(f\"  - Min corner coordinates: ({corners1[:, 0].min()}, {corners1[:, 1].min()})\")\n",
    "    print(f\"  - Max corner coordinates: ({corners1[:, 0].max()}, {corners1[:, 1].max()})\")\n",
    "\n",
    "if len(corners2) > 0:\n",
    "    print(f\"\\nImage 2 corner statistics:\")\n",
    "    print(f\"  - Min corner coordinates: ({corners2[:, 0].min()}, {corners2[:, 1].min()})\")\n",
    "    print(f\"  - Max corner coordinates: ({corners2[:, 0].max()}, {corners2[:, 1].max()})\")"
   ],
   "id": "115df3ffe6704bda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3.Use the OpenCV library to extract keypoints and compute descriptors through the function SIFT_create.detectAndCompute(). Output the number of keypoints in each image.\n",
    "You can find more details in the OpenCV documents. Note: To be efficient, we use no more than 5,000 keypoints in this case."
   ],
   "id": "5c82c49ab1e23610"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Use SIFT to extract keypoints and descriptors\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: SIFT Feature Extraction\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize the SIFT detector, limiting the number of keypoints for improved efficiency\n",
    "sift = cv2.SIFT_create(nfeatures=5000,\n",
    "                       nOctaveLayers=3,\n",
    "                       contrastThreshold=0.04,\n",
    "                       edgeThreshold=10)\n",
    "\n",
    "# Detect keypoints and compute descriptors\n",
    "print(\"Extracting SIFT features from Image 1...\")\n",
    "keypoints1, descriptors1 = sift.detectAndCompute(gray1, None)\n",
    "print(\"Extracting SIFT features from Image 2...\")\n",
    "keypoints2, descriptors2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "print(f\"\\nNumber of SIFT keypoints in image 1: {len(keypoints1)}\")\n",
    "print(f\"Number of SIFT keypoints in image 2: {len(keypoints2)}\")\n",
    "\n",
    "# Display keypoint statistical information\n",
    "if len(keypoints1) > 0:\n",
    "    scales1 = [kp.size for kp in keypoints1]\n",
    "    angles1 = [kp.angle for kp in keypoints1]\n",
    "    print(f\"\\nImage 1 Keypoint Statistics:\")\n",
    "    print(f\"  - Average scale: {np.mean(scales1):.2f}\")\n",
    "    print(f\"  - Scale range: [{min(scales1):.2f}, {max(scales1):.2f}]\")\n",
    "    print(f\"  - Average angle: {np.mean(angles1):.1f}°\")\n",
    "\n",
    "if len(keypoints2) > 0:\n",
    "    scales2 = [kp.size for kp in keypoints2]\n",
    "    angles2 = [kp.angle for kp in keypoints2]\n",
    "    print(f\"\\nImage 2 Keypoint Statistics:\")\n",
    "    print(f\"  - Average scale: {np.mean(scales2):.2f}\")\n",
    "    print(f\"  - Scale range: [{min(scales2):.2f}, {max(scales2):.2f}]\")\n",
    "    print(f\"  - Average angle: {np.mean(angles2):.1f}°\")\n",
    "\n",
    "# Draw keypoints (only display the first 100 to avoid overcrowding)\n",
    "img1_keypoints = cv2.drawKeypoints(img1, keypoints1[:100], None,\n",
    "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "img2_keypoints = cv2.drawKeypoints(img2, keypoints2[:100], None,\n",
    "                                   flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "axes[0].imshow(cv2.cvtColor(img1_keypoints, cv2.COLOR_BGR2RGB))\n",
    "axes[0].set_title(f'Image 1: {len(keypoints1)} SIFT Keypoints')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(cv2.cvtColor(img2_keypoints, cv2.COLOR_BGR2RGB))\n",
    "axes[1].set_title(f'Image 2: {len(keypoints2)} SIFT Keypoints')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display keypoint orientation distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "if len(keypoints1) > 0:\n",
    "    angles1 = [kp.angle for kp in keypoints1]\n",
    "    axes[0].hist(angles1, bins=36, range=(0, 360), alpha=0.7, color='blue')\n",
    "    axes[0].set_xlabel('Orientation (degrees)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title('Keypoint Orientations - Image 1')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "if len(keypoints2) > 0:\n",
    "    angles2 = [kp.angle for kp in keypoints2]\n",
    "    axes[1].hist(angles2, bins=36, range=(0, 360), alpha=0.7, color='red')\n",
    "    axes[1].set_xlabel('Orientation (degrees)')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Keypoint Orientations - Image 2')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "fe37a4d1288528e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4.Match the keypoints using the BFMatch() function in OpenCV.",
   "id": "ac035bae1214c89e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Match Keypoints\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: Feature Matching using BFMatcher\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create BFMatcher object (using L2 distance and cross-checking)\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "# Match descriptors\n",
    "print(\"Matching features between images...\")\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "# Sort by distance (smaller distance indicates better match)\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "print(f\"\\nTotal number of matches found: {len(matches)}\")\n",
    "print(f\"Best match distance: {matches[0].distance:.2f}\")\n",
    "print(f\"Worst match distance: {matches[-1].distance:.2f}\")\n",
    "print(f\"Median match distance: {matches[len(matches)//2].distance:.2f}\")\n",
    "print(f\"Average match distance: {np.mean([m.distance for m in matches]):.2f}\")\n",
    "\n",
    "# Analyze match distance distribution\n",
    "match_distances = [m.distance for m in matches]\n",
    "percentiles = [25, 50, 75, 90]\n",
    "percentile_values = np.percentile(match_distances, percentiles)\n",
    "print(\"\\nMatch distance percentiles:\")\n",
    "for p, v in zip(percentiles, percentile_values):\n",
    "    print(f\"  {p}th percentile: {v:.2f}\")\n",
    "\n",
    "# Draw the top 50 best matches\n",
    "print(\"\\nDisplaying top 50 matches...\")\n",
    "matched_img = cv2.drawMatches(img1, keypoints1, img2, keypoints2,\n",
    "                              matches[:50], None,\n",
    "                              flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(18, 9))\n",
    "axes.imshow(cv2.cvtColor(matched_img, cv2.COLOR_BGR2RGB))\n",
    "axes.set_title('Top 50 Feature Matches')\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display match distance distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(match_distances, bins=50, alpha=0.7, color='green')\n",
    "ax1.set_xlabel('Match Distance (lower is better)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.set_title('Distribution of Match Distances')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative Distribution Function\n",
    "sorted_distances = np.sort(match_distances)\n",
    "y_vals = np.arange(1, len(sorted_distances)+1) / len(sorted_distances)\n",
    "ax2.plot(sorted_distances, y_vals, linewidth=2, color='blue')\n",
    "ax2.set_xlabel('Match Distance')\n",
    "ax2.set_ylabel('Cumulative Probability')\n",
    "ax2.set_title('Cumulative Distribution of Match Distances')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark key percentile points\n",
    "for p, v in zip(percentiles, percentile_values):\n",
    "    ax2.axvline(x=v, color='red', linestyle='--', alpha=0.5)\n",
    "    ax2.text(v, 0.95-p/100, f'{p}%', fontsize=9, color='red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ea9e47235ea87884",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5.Compute the homography from the matched points using RANSAC. Print the Homography Matrix and explain in the notebook how RANSAC works here.",
   "id": "bf734368887efae4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute Homography Matrix using RANSAC\n",
    "\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 5: Compute Homography using RANSAC\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare coordinates of matched points\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "print(f\"Number of matched point pairs: {len(src_pts)}\")\n",
    "\n",
    "# Compute Homography Matrix using RANSAC\n",
    "# H maps dst_pts to src_pts: src_pts = H * dst_pts\n",
    "print(\"\\nRunning RANSAC to estimate homography matrix...\")\n",
    "H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC,\n",
    "                             ransacReprojThreshold=5.0,\n",
    "                             maxIters=2000,\n",
    "                             confidence=0.995)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Homography Matrix (H):\")\n",
    "print(\"=\"*40)\n",
    "print(f\"[[{H[0,0]:.6f}, {H[0,1]:.6f}, {H[0,2]:.6f}]\")\n",
    "print(f\" [{H[1,0]:.6f}, {H[1,1]:.6f}, {H[1,2]:.6f}]\")\n",
    "print(f\" [{H[2,0]:.6f}, {H[2,1]:.6f}, {H[2,2]:.6f}]]\")\n",
    "\n",
    "# Interpret the homography matrix\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"Homography Matrix Interpretation:\")\n",
    "print(\"=\"*40)\n",
    "print(\"A homography matrix H (3x3) transforms points from image 2 to image 1:\")\n",
    "print(\"  [x1]   [h11 h12 h13] [x2]\")\n",
    "print(\"  [y1] = [h21 h22 h23] [y2]\")\n",
    "print(\"  [w ]   [h31 h32 h33] [1 ]\")\n",
    "print(\"\\nWhere (x2, y2) is a point in image 2,\")\n",
    "print(\"      (x1/w, y1/w) is the corresponding point in image 1,\")\n",
    "print(\"      w = h31*x2 + h32*y2 + h33 (for normalization)\")\n",
    "\n",
    "# RANSAC statistics\n",
    "inlier_count = np.sum(mask)\n",
    "outlier_count = len(mask) - inlier_count\n",
    "inlier_ratio = inlier_count / len(mask)\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"RANSAC Results:\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Number of inliers (good matches): {inlier_count}\")\n",
    "print(f\"Number of outliers (bad matches): {outlier_count}\")\n",
    "print(f\"Inlier ratio: {inlier_ratio:.2%}\")\n",
    "print(f\"RANSAC reprojection threshold: 5.0 pixels\")\n",
    "\n",
    "# Draw inlier and outlier matches\n",
    "matches_mask = mask.ravel().tolist()\n",
    "\n",
    "# Plot match graph (green for inliers, red for outliers)\n",
    "draw_params = dict(matchColor=(0, 255, 0),  # 绿色表示inliers\n",
    "                   singlePointColor=None,\n",
    "                   matchesMask=matches_mask[:100],  # 只显示前100个匹配的状态\n",
    "                   flags=2)\n",
    "\n",
    "matched_img_with_inliers = cv2.drawMatches(img1, keypoints1, img2, keypoints2,\n",
    "                                           matches[:100], None, **draw_params)\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(18, 9))\n",
    "axes.imshow(cv2.cvtColor(matched_img_with_inliers, cv2.COLOR_BGR2RGB))\n",
    "axes.set_title(f'Matches: Green = Inliers ({inlier_count} total), Red = Outliers ({outlier_count} total)')\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize distribution of inliers\n",
    "inlier_matches = [m for i, m in enumerate(matches) if matches_mask[i] == 1]\n",
    "outlier_matches = [m for i, m in enumerate(matches) if matches_mask[i] == 0]\n",
    "\n",
    "print(f\"\\nInlier match statistics:\")\n",
    "if inlier_matches:\n",
    "    inlier_distances = [m.distance for m in inlier_matches]\n",
    "    print(f\"  - Average distance: {np.mean(inlier_distances):.2f}\")\n",
    "    print(f\"  - Min distance: {min(inlier_distances):.2f}\")\n",
    "    print(f\"  - Max distance: {max(inlier_distances):.2f}\")\n",
    "\n",
    "if outlier_matches:\n",
    "    outlier_distances = [m.distance for m in outlier_matches]\n",
    "    print(f\"\\nOutlier match statistics:\")\n",
    "    print(f\"  - Average distance: {np.mean(outlier_distances):.2f}\")\n",
    "    print(f\"  - Min distance: {min(outlier_distances):.2f}\")\n",
    "    print(f\"  - Max distance: {max(outlier_distances):.2f}\")\n",
    "\n",
    "# Visualize distance distribution of inliers and outliers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "if inlier_matches and outlier_matches:\n",
    "    inlier_distances = [m.distance for m in inlier_matches]\n",
    "    outlier_distances = [m.distance for m in outlier_matches]\n",
    "\n",
    "    axes[0].boxplot([inlier_distances, outlier_distances], labels=['Inliers', 'Outliers'])\n",
    "    axes[0].set_ylabel('Match Distance')\n",
    "    axes[0].set_title('Distance Comparison: Inliers vs Outliers')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1].hist([inlier_distances, outlier_distances], bins=30,\n",
    "                 label=['Inliers', 'Outliers'], alpha=0.7)\n",
    "    axes[1].set_xlabel('Match Distance')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Distance Distribution: Inliers vs Outliers')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "e7cda4ca11d4b256",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How RANSAC Works for Homography Estimation\n",
    "\n",
    "**RANSAC (Random Sample Consensus)** is a robust algorithm for estimating model parameters from data containing outliers. Here's how it works for homography estimation:\n",
    "\n",
    "### Step-by-Step Process:\n",
    "\n",
    "1. **Random Sampling**:\n",
    "   - Randomly select 4 point pairs from all matches (minimum needed to compute a homography)\n",
    "   - These form a \"hypothetical inlier set\"\n",
    "\n",
    "2. **Model Estimation**:\n",
    "   - Compute a homography matrix H using these 4 point pairs\n",
    "   - This is a \"hypothesis model\"\n",
    "\n",
    "3. **Model Verification**:\n",
    "   - Apply H to all points in image 2\n",
    "   - Compute reprojection error for each point:\n",
    "     ```\n",
    "     error = ||src_point - H * dst_point||\n",
    "     ```\n",
    "   - If error < threshold (5.0 pixels), mark as inlier\n",
    "\n",
    "4. **Model Evaluation**:\n",
    "   - Count number of inliers for this hypothesis\n",
    "   - More inliers = better model\n",
    "\n",
    "5. **Iteration**:\n",
    "   - Repeat steps 1-4 many times (2000 iterations here)\n",
    "   - Each iteration produces a candidate model\n",
    "\n",
    "6. **Best Model Selection**:\n",
    "   - Select the model with the most inliers\n",
    "   - This is the \"consensus set\"\n",
    "\n",
    "7. **Final Estimation**:\n",
    "   - Recompute homography using ALL inliers from the best model\n",
    "   - Uses least squares for more accurate estimation\n",
    "\n",
    "### Why RANSAC is Effective:\n",
    "\n",
    "1. **Robust to Outliers**: Can handle up to 50% outliers in data\n",
    "2. **Probability-Based**: More iterations = higher chance of finding correct model\n",
    "3. **Simple Concept**: Easy to understand and implement\n",
    "\n",
    "### Parameters Used:\n",
    "\n",
    "- **ransacReprojThreshold=5.0**: Points with reprojection error < 5 pixels are inliers\n",
    "- **maxIters=2000**: Maximum number of RANSAC iterations\n",
    "- **confidence=0.995**: Probability that at least one sample is free from outliers\n",
    "\n",
    "### Results Interpretation:\n",
    "\n",
    "- **Inliers**: Correct matches used for final homography\n",
    "- **Outliers**: Incorrect matches rejected by RANSAC\n",
    "- **Inlier Ratio**: Higher ratio indicates better overall matching quality"
   ],
   "id": "83d182714e80ef47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "6.Warp “tung_ping_chau_view2.jpeg” onto the other using the estimated transformation.\n",
    "This can be done with the perspectiveTransform() and warpPerspective() functions in OpenCV."
   ],
   "id": "7f261a303dd57b14"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Perspective Transformation and Image Stitching\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 6: Warp Image 2 onto Image 1\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get image dimensions\n",
    "h1, w1 = img1.shape[:2]\n",
    "h2, w2 = img2.shape[:2]\n",
    "\n",
    "print(f\"Image 1 dimensions: {w1} x {h1}\")\n",
    "print(f\"Image 2 dimensions: {w2} x {h2}\")\n",
    "\n",
    "# Calculate the size of the stitched image\n",
    "# Transform the four corners of image 2 to the coordinate system of image 1\n",
    "corners2 = np.float32([[0, 0], [0, h2], [w2, h2], [w2, 0]]).reshape(-1, 1, 2)\n",
    "transformed_corners = cv2.perspectiveTransform(corners2, H)\n",
    "\n",
    "print(\"\\nOriginal corners of image 2 (x, y):\")\n",
    "for i, corner in enumerate(corners2.reshape(-1, 2)):\n",
    "    print(f\"  Corner {i}: ({corner[0]:.1f}, {corner[1]:.1f})\")\n",
    "\n",
    "print(\"\\nTransformed corners of image 2 (in image 1 coordinate system):\")\n",
    "for i, corner in enumerate(transformed_corners.reshape(-1, 2)):\n",
    "    print(f\"  Corner {i}: ({corner[0]:.1f}, {corner[1]:.1f})\")\n",
    "\n",
    "# Find the bounding box of all points (image 1 corners + transformed image 2 corners)\n",
    "all_corners = np.concatenate((\n",
    "    np.float32([[0, 0], [0, h1], [w1, h1], [w1, 0]]).reshape(-1, 1, 2),\n",
    "    transformed_corners\n",
    "), axis=0)\n",
    "\n",
    "[x_min, y_min] = np.int32(all_corners.min(axis=0).ravel() - 0.5)\n",
    "[x_max, y_max] = np.int32(all_corners.max(axis=0).ravel() + 0.5)\n",
    "\n",
    "print(f\"\\nBounding box of all corners:\")\n",
    "print(f\"  Min: ({x_min}, {y_min})\")\n",
    "print(f\"  Max: ({x_max}, {y_max})\")\n",
    "\n",
    "# Calculate translation to bring all points to positive coordinates\n",
    "translation = [-x_min, -y_min]\n",
    "H_translation = np.array([[1, 0, translation[0]],\n",
    "                          [0, 1, translation[1]],\n",
    "                          [0, 0, 1]])\n",
    "\n",
    "# Apply translation to the homography matrix\n",
    "H_final = H_translation @ H\n",
    "\n",
    "print(f\"\\nTranslation vector: ({translation[0]}, {translation[1]})\")\n",
    "\n",
    "# Calculate stitched image dimensions\n",
    "result_width = x_max - x_min\n",
    "result_height = y_max - y_min\n",
    "print(f\"\\nStitched image will be: {result_width} x {result_height} pixels\")\n",
    "\n",
    "# Transform the second image\n",
    "print(\"\\nWarping image 2 using the homography matrix...\")\n",
    "warped_img2 = cv2.warpPerspective(img2, H_final, (result_width, result_height))\n",
    "\n",
    "# Create stitched image (place image 1 at correct position)\n",
    "stitched_img = np.zeros((result_height, result_width, 3), dtype=np.uint8)\n",
    "stitched_img[translation[1]:translation[1] + h1,\n",
    "             translation[0]:translation[0] + w1] = img1\n",
    "\n",
    "print(f\"Image 1 placed at position: ({translation[0]}, {translation[1]})\")\n",
    "\n",
    "# Simple overlay blending\n",
    "print(\"\\nPerforming simple overlay blending...\")\n",
    "# Find non-black areas in warped_img2\n",
    "mask_warped = cv2.cvtColor(warped_img2, cv2.COLOR_BGR2GRAY)\n",
    "mask_warped = (mask_warped > 10).astype(np.uint8) * 255\n",
    "\n",
    "# Copy warped_img2 to stitched_img using mask\n",
    "stitched_img_simple = stitched_img.copy()\n",
    "for c in range(3):\n",
    "    stitched_img_simple[:, :, c] = stitched_img[:, :, c] * (1 - mask_warped/255) + \\\n",
    "                                   warped_img2[:, :, c] * (mask_warped/255)\n",
    "\n",
    "# Calculate overlap area\n",
    "overlap_mask = (mask_warped > 0) & (stitched_img.sum(axis=2) > 0)\n",
    "overlap_area = np.sum(overlap_mask)\n",
    "print(f\"Overlap area between images: {overlap_area} pixels\")\n",
    "\n",
    "# Display intermediate results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Original images\n",
    "axes[0, 0].imshow(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 0].set_title('Image 1 (Reference)')\n",
    "axes[0, 0].axis('off')\n",
    "\n",
    "axes[0, 1].imshow(cv2.cvtColor(img2, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 1].set_title('Image 2 (To be warped)')\n",
    "axes[0, 1].axis('off')\n",
    "\n",
    "# Transformed image\n",
    "axes[0, 2].imshow(cv2.cvtColor(warped_img2, cv2.COLOR_BGR2RGB))\n",
    "axes[0, 2].set_title('Image 2 (After Warping)')\n",
    "axes[0, 2].axis('off')\n",
    "\n",
    "# Stitching canvas\n",
    "axes[1, 0].imshow(cv2.cvtColor(stitched_img, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 0].set_title('Canvas with Image 1 Placed')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Mask\n",
    "axes[1, 1].imshow(mask_warped, cmap='gray')\n",
    "axes[1, 1].set_title('Mask for Warped Image 2')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "# Simple stitching result\n",
    "axes[1, 2].imshow(cv2.cvtColor(stitched_img_simple, cv2.COLOR_BGR2RGB))\n",
    "axes[1, 2].set_title('Simple Stitching Result')\n",
    "axes[1, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "ff463612c6197d99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "7.Create the new image and show the result.",
   "id": "b680221ff5def578"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final Image Stitching and Result\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 7: Final Image Stitching\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply improved blending method (using the same function as before)\n",
    "def improved_blending(img1, img2_warped):\n",
    "    \"\"\"Improved gradient blending method\"\"\"\n",
    "    gray_warped = cv2.cvtColor(img2_warped, cv2.COLOR_BGR2GRAY)\n",
    "    mask = (gray_warped > 10).astype(np.float32)\n",
    "\n",
    "    kernel_size = 51\n",
    "    mask_blurred = cv2.GaussianBlur(mask, (kernel_size, kernel_size), 0)\n",
    "    mask_blurred = np.clip(mask_blurred, 0, 1)\n",
    "\n",
    "    img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "    img1_mask = (img1_gray > 10).astype(np.float32)\n",
    "\n",
    "    overlap = (mask_blurred > 0) & (img1_mask > 0)\n",
    "\n",
    "    if np.any(overlap):\n",
    "        dist_transform = cv2.distanceTransform((img1_mask * 255).astype(np.uint8),\n",
    "                                              cv2.DIST_L2, 5)\n",
    "        dist_transform = dist_transform / (dist_transform.max() + 1e-6)\n",
    "        mask_blurred[overlap] = dist_transform[overlap]\n",
    "\n",
    "    mask_3ch = np.stack([mask_blurred, mask_blurred, mask_blurred], axis=2)\n",
    "    result = img1.astype(np.float32) * (1 - mask_3ch) + img2_warped.astype(np.float32) * mask_3ch\n",
    "    return result.astype(np.uint8)\n",
    "\n",
    "# Create final stitched image\n",
    "stitched_img_blended = improved_blending(stitched_img, warped_img2)\n",
    "\n",
    "# Calculate basic quality metrics (optional)\n",
    "def calculate_stitching_quality(stitched_img):\n",
    "    gray = cv2.cvtColor(stitched_img, cv2.COLOR_BGR2GRAY)\n",
    "    non_black_pixels = np.sum(gray > 10)\n",
    "    total_pixels = gray.size\n",
    "    coverage = non_black_pixels / total_pixels\n",
    "    return coverage\n",
    "\n",
    "coverage = calculate_stitching_quality(stitched_img_blended)\n",
    "print(f\"Image coverage: {coverage:.2%}\")\n",
    "\n",
    "# ============================================================\n",
    "# DISPLAY AND SAVE FINAL RESULT (ONLY ONE IMAGE)\n",
    "# ============================================================\n",
    "\n",
    "# Display final stitched image\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.imshow(cv2.cvtColor(stitched_img_blended, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Final Stitched Panorama', fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save the final image\n",
    "output_filename = 'tung_ping_chau_panorama.jpg'\n",
    "cv2.imwrite(output_filename, stitched_img_blended)\n",
    "print(f\"\\n✓ Final image saved as: '{output_filename}'\")\n",
    "print(f\"  - Dimensions: {stitched_img_blended.shape[1]} x {stitched_img_blended.shape[0]} pixels\")\n",
    "print(f\"  - Format: BGR (OpenCV native)\")\n",
    "\n",
    "# Verify file was saved\n",
    "import os\n",
    "if os.path.exists(output_filename):\n",
    "    file_size = os.path.getsize(output_filename) / 1024\n",
    "    print(f\"  - File size: {file_size:.1f} KB\")\n",
    "\n",
    "    # Optional: Verify by loading and checking dimensions\n",
    "    saved_img = cv2.imread(output_filename)\n",
    "    if saved_img is not None and saved_img.shape == stitched_img_blended.shape:\n",
    "        print(f\"  - Verification: File saved successfully and dimensions match\")\n",
    "    else:\n",
    "        print(f\"  - Verification: File saved but dimensions may differ due to compression\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMAGE STITCHING ASSIGNMENT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nSummary of steps completed:\")\n",
    "print(\"1. ✓ Images loaded and converted to grayscale\")\n",
    "print(\"2. ✓ Harris corner points detected\")\n",
    "print(\"3. ✓ SIFT keypoints extracted (limited to 5000)\")\n",
    "print(\"4. ✓ Keypoints matched using BFMatcher\")\n",
    "print(\"5. ✓ Homography matrix computed using RANSAC\")\n",
    "print(\"6. ✓ Image 2 warped onto Image 1\")\n",
    "print(f\"7. ✓ Final panorama created and saved as '{output_filename}'\")\n",
    "print(\"\\n\" + \"=\"*70)"
   ],
   "id": "889f5ae81b2b8391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: By default, OpenCV uses the BGR (Blue-Green-Red) color channel order, which is a historical legacy related to the BGR arrangement of early camera sensors. In contrast, Matplotlib and most image viewers use the RGB (Red-Green-Blue) order—a more modern and universal standard. This inconsistency in color channel ordering may occur, so a simple conversion might be necessary when outputting or saving the final stitched image.",
   "id": "177ba47dc1048a5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
